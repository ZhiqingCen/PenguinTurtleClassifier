{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Preprocessing\n",
    "\n",
    "- usage: call function `generate_new_train_img(train_path)`, where train_path is the directory of train images\n",
    "- new train_annotation json file and new images will be generated to a directory 'new'\n",
    "  - google drive download link\n",
    "  - [version1](https://drive.google.com/file/d/1vKhKZugDBe11JA9ss6sJ9U1StgN681bw/view?usp=sharing) - generating 9 new augmentated image for each train images, updated train_annotation with labels and recalculate bounding boxes.\n",
    "  - [version2](https://drive.google.com/file/d/14VIAkTLwMEs-f0MII-YHXAXDQWtIy1ig/view?usp=sharing) - apart from version1, 50 images is moved to new directory for validation without generating any image augmentation new images\n",
    "  - [version3](https://drive.google.com/file/d/1Vd9zstfV_TO1mPygXt3jhkfxmiZVo7LD/view?usp=sharing) - apart from version2, bbox data in generated annotations files are converted back to `[xmin, ymin, width, height]` format\n",
    "  - [version4](https://drive.google.com/file/d/13d7a9JlNWWpmOvTplTG-gbraqB6IqwM9/view?usp=sharing) - apart from version3, images are moved to new directory\n",
    "    - `new/penguin` contains original penguin images (excluding those images in `valid_penguin`) and augmentation images based on these original images\n",
    "    - `new/turtle` contains original penguin images (excluding those images in `valid_penguin`) and augmentation images based on these original images\n",
    "    - `new/valid_penguin` contains 25 randomly selected penguin images for validation purpose without any augmentation\n",
    "    - `new/valid_turtle` contains 25 randomly selected turtle images for validation purpose without any augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from random import randint, seed\n",
    "from PIL import Image, ImageChops\n",
    "import torch\n",
    "from torchvision.transforms import ToPILImage, ColorJitter\n",
    "from torchvision.transforms.functional import invert, posterize, solarize, hflip, vflip\n",
    "from torchvision.utils import draw_bounding_boxes\n",
    "from torchvision.io import read_image"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Annotation File\n",
    "- id: integer, id #\n",
    "- image_id: integer, image id #\n",
    "- category_id: 0 for background, 1 for penguin, 2 for turtle\n",
    "- bbox: list of integers representing the bounding box coordinates in Pascal VOC format [xmin, ymin, xmax, ymax]\n",
    "- area: integer representing area of bounding box.\n",
    "- segmentation: empty list; add segmentation masks if you'd like!\n",
    "- iscrowd: integer 0 or 1; whether the instance is a crowd or individual. Not relevant to this particular use case, but is a necessary key for some models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of category_id 0 (background) = 0\n",
      "   image_id  category_id                  bbox    area     label\n",
      "0         0            0   [119, 25, 205, 606]  124230  original\n",
      "1         1            0   [131, 82, 327, 440]  143880  original\n",
      "2         2            1  [225, 298, 198, 185]   36630  original\n",
      "3         3            1  [468, 109, 172, 193]   33196  original\n",
      "4         4            0   [14, 242, 611, 154]   94094  original\n"
     ]
    }
   ],
   "source": [
    "def clean_annotations():\n",
    "    train_annotations = pd.read_json('train_annotations')\n",
    "\n",
    "    # remove redundant data\n",
    "    train_annotations = train_annotations.drop(['id', 'segmentation','iscrowd'], axis=1)\n",
    "\n",
    "    # category_id: background = 0 penguin = 1, turtle = 2\n",
    "    category_background = train_annotations[train_annotations['category_id'] == 0]\n",
    "    print(f'number of category_id 0 (background) = {len(category_background)}')\n",
    "    # no rows with category_id = 0, make it binary, now penguin = 0, turtle = 1\n",
    "    train_annotations['category_id'] = train_annotations['category_id'].replace({1:0, 2:1})\n",
    "\n",
    "    # add new column for preprocessing labels\n",
    "    train_annotations['label'] = 'original'\n",
    "\n",
    "    print(train_annotations.head())\n",
    "    return train_annotations\n",
    "\n",
    "_ = clean_annotations()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GenerateImage:\n",
    "    def __init__(self, img, image_id, category_id, bbox, area):\n",
    "        self.img = img\n",
    "        self.image_id = image_id\n",
    "        self.category_id = category_id\n",
    "        self.bbox = bbox\n",
    "        self.height = 640 # height\n",
    "        self.width = 640 # width\n",
    "        self.area = area\n",
    "        self.color_jitter = ColorJitter(saturation=.5, hue=.3) # satuation, contrast, brightness=.7\n",
    "        self.path = ''\n",
    "        self.which_path()\n",
    "    \n",
    "    def which_path(self):\n",
    "        if self.category_id == 0:\n",
    "            self.path = 'new/penguin/'\n",
    "        else:\n",
    "            self.path = 'new/turtle/'\n",
    "\n",
    "    def jitter_image(self, index):\n",
    "        new_img = self.color_jitter(self.img)\n",
    "        new_img.save(f'{self.path}image_id_{index}.jpg')\n",
    "        return [index, self.category_id, self.bbox, self.area, 'jitter']\n",
    "\n",
    "    def inverted_image(self, index):\n",
    "        new_img = invert(self.img)\n",
    "        new_img.save(f'{self.path}image_id_{index}.jpg')\n",
    "        return [index, self.category_id, self.bbox, self.area, 'invert']\n",
    "\n",
    "    def posterized_image(self, index):\n",
    "        new_img = posterize(self.img, bits=3)\n",
    "        new_img.save(f'{self.path}image_id_{index}.jpg')\n",
    "        return [index, self.category_id, self.bbox, self.area, 'posterize']\n",
    "\n",
    "    def solarized_image(self, index):\n",
    "        # not using this method\n",
    "        new_img = solarize(self.img, threshold=210) # 192, 240\n",
    "        new_img.save(f'{self.path}image_id_{index}.jpg')\n",
    "        return [index, self.category_id, self.bbox, self.area, 'solarize']\n",
    "\n",
    "    def rotate_90_degree_image(self, index):\n",
    "        new_img = self.img.rotate(90)\n",
    "        new_img.save(f'{self.path}image_id_{index}.jpg')\n",
    "        # new_bbox = [int(self.bbox[1]), self.height-int(self.bbox[2]), int(self.bbox[3]), self.height-int(self.bbox[0])]\n",
    "        new_bbox = [int(self.bbox[1]), self.width-(int(self.bbox[0])+int(self.bbox[2])), int(self.bbox[3]), int(self.bbox[2])]\n",
    "        return [index, self.category_id, new_bbox, self.area, 'rotate90']\n",
    "      \n",
    "    def rotate_180_degree_image(self, index):\n",
    "        new_img = self.img.rotate(180)\n",
    "        new_img.save(f'{self.path}image_id_{index}.jpg')\n",
    "        # new_bbox = [self.width-int(self.bbox[2]), self.height-int(self.bbox[3]), self.width-int(self.bbox[0]), self.height-int(self.bbox[1])]\n",
    "        new_bbox = [self.width-(int(self.bbox[0])+int(self.bbox[2])), self.height-(int(self.bbox[1])+int(self.bbox[3])), int(self.bbox[2]), int(self.bbox[3])]\n",
    "        return [index, self.category_id, new_bbox, self.area, 'rotate180']\n",
    " \n",
    "    def rotate_270_degree_image(self, index):\n",
    "        new_img = self.img.rotate(270)\n",
    "        new_img.save(f'{self.path}image_id_{index}.jpg')\n",
    "        # new_bbox = [self.width-int(self.bbox[3]), int(self.bbox[0]), self.width-int(self.bbox[1]), int(self.bbox[2])]\n",
    "        new_bbox = [self.width-(int(self.bbox[1])+int(self.bbox[3])), int(self.bbox[0]), int(self.bbox[3]), int(self.bbox[2])]\n",
    "        return [index, self.category_id, new_bbox, self.area, 'rotate270']\n",
    "\n",
    "    def horizontally_flip_image(self, index):\n",
    "        new_img = hflip(self.img)\n",
    "        new_img.save(f'{self.path}image_id_{index}.jpg')\n",
    "        # new_bbox = [self.width-int(self.bbox[2]), int(self.bbox[1]), self.width-int(self.bbox[0]), int(self.bbox[3])]\n",
    "        new_bbox = [self.width-(int(self.bbox[0])+int(self.bbox[2])), int(self.bbox[1]), int(self.bbox[2]), int(self.bbox[3])]\n",
    "        return [index, self.category_id, new_bbox, self.area, 'hflip']\n",
    "\n",
    "    def vertically_flip_image(self, index):\n",
    "        new_img = vflip(self.img)\n",
    "        new_img.save(f'{self.path}image_id_{index}.jpg')\n",
    "        # new_bbox = [int(self.bbox[0]), self.height-int(self.bbox[3]), int(self.bbox[2]), self.height-int(self.bbox[1])]\n",
    "        new_bbox = [int(self.bbox[0]), self.height-(int(self.bbox[1])+int(self.bbox[3])), int(self.bbox[2]), int(self.bbox[3])]\n",
    "        return [index, self.category_id, new_bbox, self.area, 'vflip']\n",
    "\n",
    "    def jitter_vertically_flip_image(self, index):\n",
    "        new_img = vflip(self.color_jitter(self.img))\n",
    "        new_img.save(f'{self.path}image_id_{index}.jpg')\n",
    "        # new_bbox = [int(self.bbox[0]), self.height-int(self.bbox[3]), int(self.bbox[2]), self.height-int(self.bbox[1])]\n",
    "        new_bbox = [int(self.bbox[0]), self.height-(int(self.bbox[1])+int(self.bbox[3])), int(self.bbox[2]), int(self.bbox[3])]\n",
    "        return [index, self.category_id, new_bbox, self.area, 'jitter_vflip']\n",
    "\n",
    "    def jitter_180_degree_image(self, index):\n",
    "        new_img = self.color_jitter(self.img).rotate(180)\n",
    "        new_img.save(f'{self.path}image_id_{index}.jpg')\n",
    "        # new_bbox = [self.width-int(self.bbox[2]), self.height-int(self.bbox[3]), self.width-int(self.bbox[0]), self.height-int(self.bbox[1])]\n",
    "        new_bbox = [self.width-(int(self.bbox[0])+int(self.bbox[2])), self.height-(int(self.bbox[1])+int(self.bbox[3])), int(self.bbox[2]), int(self.bbox[3])]\n",
    "        return [index, self.category_id, new_bbox, self.area, 'jitter180']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bbox(test, original, train_annotations):\n",
    "    bbox = train_annotations.iloc[test]['bbox']\n",
    "    bbox = [bbox[0], bbox[1], bbox[2]+bbox[0], bbox[3]+bbox[1]]\n",
    "    bbox = torch.tensor(bbox, dtype=torch.int)\n",
    "    bbox = bbox.unsqueeze(0)\n",
    "    img = read_image(f'new/turtle/image_id_{test}.jpg')\n",
    "    new_img = draw_bounding_boxes(img, boxes=bbox, width=3, colors=(255, 255, 0))\n",
    "    new_img = ToPILImage()(new_img)\n",
    "    new_img.show()\n",
    "\n",
    "    bbox1 = train_annotations.iloc[original]['bbox']\n",
    "    bbox1 = [bbox1[0], bbox1[1], bbox1[2]+bbox1[0], bbox1[3]+bbox1[1]]\n",
    "    bbox1 = torch.tensor(bbox1, dtype=torch.int)\n",
    "    bbox1 = bbox1.unsqueeze(0)\n",
    "    print(bbox1)\n",
    "    img1 = read_image(f'new/turtle/image_id_{original}.jpg')\n",
    "    new_img1 = draw_bounding_boxes(img1, boxes=bbox1, width=3, colors=(255, 255, 0))\n",
    "    new_img1 = ToPILImage()(new_img1)\n",
    "    new_img1.show()\n",
    "\n",
    "# train_annotations = pd.read_json('new/train_annotations')\n",
    "# test_bbox(test=1697, original=517, train_annotations=train_annotations)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "references: [Pytorch ILLUSTRATION OF TRANSFORMS](https://pytorch.org/vision/stable/auto_examples/plot_transforms.html#sphx-glr-auto-examples-plot-transforms-py)\n",
    "\n",
    "- The **ColorJitter** transform randomly changes the brightness, saturation, and other properties of an image.\n",
    "- The **RandomInvert** transform randomly inverts the colors of the given image.\n",
    "  - since it's random, it have high change to no modify the original image results in large number of duplicate images\n",
    "  - use [invert](https://pytorch.org/vision/stable/generated/torchvision.transforms.functional.invert.html#torchvision.transforms.functional.invert) instead\n",
    "- The RandomPosterize transform (see also posterize()) randomly posterizes the image by reducing the number of bits of each color channel.\n",
    "  - use [posterize](https://pytorch.org/vision/stable/generated/torchvision.transforms.functional.posterize.html#torchvision.transforms.functional.posterize) instead\n",
    "- The RandomSolarize transform (see also solarize()) randomly solarizes the image by inverting all pixel values above the threshold.\n",
    "  - use [solarize](https://pytorch.org/vision/stable/generated/torchvision.transforms.functional.solarize.html#torchvision.transforms.functional.solarize) insted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_new_train_img(train_path):\n",
    "    train_annotations = clean_annotations()\n",
    "    valid_annotations = pd.DataFrame(columns=['image_id', 'category_id', 'bbox', 'area', 'label'])\n",
    "    print(train_annotations[train_annotations['image_id'] == 2])\n",
    "\n",
    "    index = 500\n",
    "    num_train_files = 500\n",
    "    test_penguin_counter = 0\n",
    "    test_turtle_counter = 0\n",
    "    penguin_path = 'new/penguin/'\n",
    "    turtle_path = 'new/turtle/'\n",
    "    seed(5)\n",
    "\n",
    "    for i in range(num_train_files):\n",
    "        img = Image.open(f'{train_path}/image_id_{i:03}.jpg')\n",
    "\n",
    "        category_id = train_annotations.iloc[i]['category_id']\n",
    "        area = train_annotations.iloc[i]['area']\n",
    "        bbox = train_annotations.iloc[i]['bbox']\n",
    "\n",
    "        valid_path = 'new/valid_'\n",
    "\n",
    "        # randomly choose 25:25 images for validation only\n",
    "        if randint(1, 7) == 3:\n",
    "            if (category_id == 0 and test_penguin_counter < 25):\n",
    "                test_penguin_counter += 1\n",
    "                img.save(f'{valid_path}penguin/image_id_{i}.jpg')\n",
    "                valid_annotations.loc[len(valid_annotations)] = [i, category_id, bbox, area, 'validation']\n",
    "                continue\n",
    "            elif (category_id == 1 and test_turtle_counter < 25):\n",
    "                test_turtle_counter += 1\n",
    "                img.save(f'{valid_path}turtle/image_id_{i}.jpg')\n",
    "                valid_annotations.loc[len(valid_annotations)] = [i, category_id, bbox, area, 'validation']\n",
    "                continue\n",
    "        \n",
    "        if category_id == 0:\n",
    "            img.save(f'{penguin_path}image_id_{i}.jpg')\n",
    "        else:\n",
    "            img.save(f'{turtle_path}image_id_{i}.jpg')\n",
    "\n",
    "        generator = GenerateImage(img, i, category_id, bbox, area)\n",
    "        train_annotations.loc[index] = generator.jitter_image(index)\n",
    "        index += 1\n",
    "        train_annotations.loc[index] = generator.inverted_image(index)\n",
    "        index += 1\n",
    "        train_annotations.loc[index] = generator.posterized_image(index)\n",
    "        index += 1\n",
    "        train_annotations.loc[index] = generator.rotate_90_degree_image(index)\n",
    "        index += 1\n",
    "        train_annotations.loc[index] = generator.rotate_180_degree_image(index)\n",
    "        index += 1\n",
    "        train_annotations.loc[index] = generator.rotate_270_degree_image(index)\n",
    "        index += 1\n",
    "        train_annotations.loc[index] = generator.horizontally_flip_image(index)\n",
    "        index += 1\n",
    "        train_annotations.loc[index] = generator.jitter_vertically_flip_image(index)\n",
    "        index += 1\n",
    "        train_annotations.loc[index] = generator.jitter_180_degree_image(index)\n",
    "        index += 1\n",
    "    \n",
    "    # remove redundant rows from train_annotations\n",
    "    for i in valid_annotations.index:\n",
    "        image_id = valid_annotations['image_id'][i]\n",
    "        condition = train_annotations[train_annotations['image_id'] == image_id].index\n",
    "        train_annotations.drop(condition, inplace=True)\n",
    "\n",
    "    train_annotations.to_json('new/train_annotations', orient='records') # , orient = 'split', compression = 'infer'\n",
    "    valid_annotations.to_json('new/valid_annotations', orient='records')\n",
    "    return train_annotations, valid_annotations\n",
    "\n",
    "# train_annotations, valid_annotations = generate_new_train_img('train')\n",
    "# test_bbox(test=512, original=2, train_annotations=train_annotations)\n",
    "# print(valid_annotations.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      image_id  category_id                  bbox    area         label\n",
      "4046      4096            1  [150, 142, 477, 258]  123066     rotate180\n",
      "4047      4097            1   [142, 13, 258, 477]  123066     rotate270\n",
      "4048      4098            1  [150, 240, 477, 258]  123066         hflip\n",
      "4049      4099            1   [13, 142, 477, 258]  123066  jitter_vflip\n",
      "4050      4100            1  [150, 142, 477, 258]  123066     jitter180\n"
     ]
    }
   ],
   "source": [
    "train_annotations = pd.read_json('new/train_annotations')\n",
    "print(train_annotations.tail())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1.], device='mps:0')\n"
     ]
    }
   ],
   "source": [
    "if torch.backends.mps.is_available():\n",
    "    mps_device = torch.device(\"mps\")\n",
    "    x = torch.ones(1, device=mps_device)\n",
    "    print (x)\n",
    "else:\n",
    "    print (\"MPS device not found.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of category_id 0 (background) = 0\n",
      "   image_id  category_id                  bbox    area     label\n",
      "0         0            0   [119, 25, 205, 606]  124230  original\n",
      "1         1            0   [131, 82, 327, 440]  143880  original\n",
      "2         2            1  [225, 298, 198, 185]   36630  original\n",
      "3         3            1  [468, 109, 172, 193]   33196  original\n",
      "4         4            0   [14, 242, 611, 154]   94094  original\n",
      "[<PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x640 at 0x17BA77D30>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x640 at 0x17BA75600>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x640 at 0x17BA74970>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x640 at 0x17BA77CA0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x640 at 0x17BA77160>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x640 at 0x17BA77130>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x640 at 0x17BA771F0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x640 at 0x17BA76EC0>, <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=640x640 at 0x17BA76E30>]\n"
     ]
    }
   ],
   "source": [
    "# function written based on request after 2 weeks of the original code pushed to Github\n",
    "# written in a way such that no modification to original code is needed\n",
    "def preprocessing_demo(image, image_id):\n",
    "    # input image is in PIL.Image format\n",
    "    image_path = 'new/penguin/' # use this if input penguin image\n",
    "    # image_path = 'new/turtle/' # use this if input turtle image\n",
    "\n",
    "    ret = []\n",
    "    # not sure which image and directory you would be using so i made the indices \n",
    "    # large such that it won't overwtire any existing images to generate possible error\n",
    "    files = [10001, 10002, 10003, 10004, 10005, 10006, 10007, 10008, 10009]\n",
    "    train_annotations = clean_annotations()\n",
    "    category_id = train_annotations.iloc[image_id]['category_id']\n",
    "    area = train_annotations.iloc[image_id]['area']\n",
    "    bbox = train_annotations.iloc[image_id]['bbox']\n",
    "\n",
    "    generator = GenerateImage(image, image_id, category_id, bbox, area)\n",
    "    _ = generator.jitter_image(files[0])\n",
    "    _ = generator.inverted_image(files[1])\n",
    "    _ = generator.posterized_image(files[2])\n",
    "    _ = generator.rotate_90_degree_image(files[3])\n",
    "    _ = generator.rotate_180_degree_image(files[4])\n",
    "    _ = generator.rotate_270_degree_image(files[5])\n",
    "    _ = generator.horizontally_flip_image(files[6])\n",
    "    _ = generator.jitter_vertically_flip_image(files[7])\n",
    "    _ = generator.jitter_180_degree_image(files[8])\n",
    "\n",
    "    for file in files:\n",
    "        try:\n",
    "            ret.append(Image.open(f'{image_path}/image_id_{file}.jpg'))\n",
    "        except IOError:\n",
    "            print(f'Error: unable to open file {image_path}/image_id_{file}.jpg')\n",
    "\n",
    "    return ret\n",
    "\n",
    "image = Image.open(f'new/penguin/image_id_0.jpg')\n",
    "print(preprocessing_demo(image, 0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Anaconda (cv)",
   "language": "python",
   "name": "anaconda-cv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
